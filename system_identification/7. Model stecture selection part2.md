# MODEL DEVELOPMENT

This is the primal objective of identification. It consists of three main operations:
1. Selecting a model structure
2. Estimating the chosen model
3. Quality assessment of the model
The goal of modeling is to obtain the most parsimonious model that has the best properties and with the least modeling effort.
Parsimony of the model usually refers to parameter dimensionality. Attractive properties include the usual good predictive ability, high model accuracy and precision, while the modeling effort mainly appeals to the computational complexity of the estimation algorithm. As we have learnt through the theoretical discussions and illustrative examples in this text, these requirements are usually conflicting in nature. The final model is therefore always governed by some trade-offs determined by the application.

## MODEL STRUCTURE SELECTION

Selecting a model structure involves making decisions on:
i. Model type: LTI / LTV / Non-linear, etc.
ii. Domain of modeling: Time, frequency, time-frequency or any other basis space
iii. Model family: ARX, ARMAX, OE, non-linear classes, etc.
iv. Model characteristics: Order and input memory

> **Special Note:**
> 
> Start with a simple model, preferably guided by non-parametric analysis and (possibly some) a priori knowledge, and proceed towards more sophisticated model structures guided by data and model diagnostics.

### Model type and family

LTI models are advantageous in several ways, unless the process is known to exhibit strong non-linearity (e.g., pH systems) and time-varying behavior in the operating regime, it is advisable to start with a LTI model.
Time-domain models are obviously best suited for predictions, but frequency-domain models are very useful in capturing important process characteristics. The domain of the final model is usually driven by the application.
Different factors that influence the choice of candidate LTI parametric models:

1. **Accuracy (bias) and precision (variance) requirements**: An important aspect is the interplay between the stochastic and deterministic parts of the model. The structure of the stochastic part can significantly alter the bias (accuracy) and variance (precision) of the deterministic part. The OE model produces the best approximation of the plant model for a given input, whereas the linear-predictor ARX model gives importance to approximation errors in frequency ranges outside the bandwidth of the plant. Finally, if data is filtered, the role of pre-filter should also be taken into account.

2. **Prediction accuracy and horizon**: The horizon over which the prediction is sought has a significant impact on the model quality. As we have learnt earlier, minimizing one-step and infinite-step ahead predictions (the case of OE model) produce models with different predictive abilities. Unless otherwise explicitly demanded by the application, the one-step ahead prediction is of interest.

3. **End-use of the model**: The end application usually influences the choice of candidate as well as the final model. Control applications are typically content with lower-order models whereas simulation and design applications demand fine grain models.

4. **Estimation aspects**: Models that yield linear-in-parameter predictors are naturally preferred to those that result in non-linear predictors, unless there is a compelling reason to choose otherwise (for example, end-use). Recall that LTI deterministic-plus-stochastic models do not necessarily produce predictors that are linear in parameters unlike pure LTI deterministic models, with the exception of ARX / FIR model structure.

5. **Prior knowledge**: As we have noted on several occasions, prior knowledge is useful in restricting the search to a smaller class of models or in producing physically meaningful models.

### Modeling estimation and effort

In setting up the estimation problem, the following points should be given due consideration:

- **End-use of the model**: The general requirement is that the one-step ahead prediction errors be minimum. However, the end-use of the model may require minimization of p-step ahead predictions. The loss function should be formulated accordingly.

- **Domain of estimation**: It may be easier to estimate the parameters in a domain different from that of the measurements. Moreover, it may be easier to derive the theoretical bounds on the errors in a different domain. In many applications (e.g., vibration machinery, acoustics) processes are naturally characterized in frequency-domain, making it therefore the natural choice for modeling.

- **Estimator complexity vs. quality**: Estimation methods are characterized by the errors in the estimates they produce. Likelihood methods are generally known to be the best estimators with large sample sizes. However, the associated optimization problem is non-linear and can be computationally demanding. When an estimator such as the LS estimator, which may produce estimates with relatively larger theoretical error but is easier on the computation, is available, it may be the preferred choice.

### OPTIONS IN PARAMETRIC MODELING

In estimating the parametric model, the user has the freedom to make decisions on a few factors, namely:

1. Initial structure and conditions (for output and input signals)
2. Data pre-filtering (already addressed)
3. Initial guesses for model parameters (with non-linear predictors)
4. Regularization
5. Model order

The following sections discuss the remaining aspects.

#### Initial structure and conditions

What is a reasonable starting parametric model structure for a given process? While several choices exist, two prominent ones are the equation-error (ARX) and the output-error (OE) structure as illustrated in the liquid level case study of Chapter 2. An advantage of the ARX model structure is that one can easily search over different orders in a computationally efficient manner (see ยง21.6.1.1). OE model structures, on the other hand, result in non-linear predictors increasing the computational burden. However, they result in best estimates of plant models (under open-loop conditions). Moreover, the latter approach can be applied to a broader class of problems whereas the ARX structures are capable of modeling a restricted class. The case studies in Chapter 24 illustrate these two different approaches.

Recall from Chapter 21 that the regressor vector $$\varphi[k]$$ involved in the estimation of parametric models generally consists of past outputs, inputs and possibly prediction errors. This information is unavailable for $$k < 0$$. In other words, the regressors cannot be constructed for $$k < \max(n_a, n_b + n_k)$$, in the case of ARX models or for $$k < \max(n_b + n_k, n_f)$$ for OE models and so on. In general, denote the critical instant at which the regressor vector to be fully available by $$k_c$$. There are three possible approaches that one could adopt:

1. **Conditional approach**: Evaluate the loss function for instants $$k \geq k_c$$. The traditional implementation of the OLS method for estimating ARX models is an example of this approach (recall Algorithm 21.3). This strategy amounts to assuming the outputs at $$k < k_c$$ to be fixed at their given values or that we sacrifice that many samples. Computationally this is the lightest among the three approaches, but the sacrifice can be significant when the $$N-p$$ is small, where $$p = \dim \theta$$.

2. **Unconditional approach**: The strategy here is similar to that used in MLE or the unconditional LS technique. Set the prediction errors to zero for $$k < k_c$$, but take into account the randomness in the initial values. Relative to the conditional case, this strategy is computationally heavy, but the resulting estimates have better properties.

3. **Prior fixation/estimation**: Here, the unknown initial conditions, i.e., at negative instants, are either fixed or estimated (for example, as unknown parameters using a PEM method or as initial conditions of states using a Kalman filter).

### PRELIMINARY ESTIMATES OF MODELS

With the exception of ARX (and FIR models), PEM estimation of all other model structures calls for non-linear optimization techniques (recall ยง21.6). The goodness of the resulting estimates crucially depends on the quality of initial guesses. The methods discussed in previous chapters can be used to fetch good preliminary estimates. Given below are some common initialization methods for specific model structures:

1. **ARMAX models**: 
   - Use the multistage IV4 method described in ยง21.7.1. 
  ```matlab
  % Using IV4 method
  armax_iv4 = iv4('armax', data, [na nb nc nk]);
  ```
   - Alternatively, the PLR method summarized in Algorithm 21.5 may be used.
  ```matlab
  % Using IV4 method
  armax_plr = plsregress('armax', data, [na nb nc nk]); ?
  ```

2. **OE models**: 
   - Since the OE structure is a special case of ARMAX, one can use the same initialization algorithms as for ARMAX structure. 
   - In addition, the Steiglitz-McBride method (Algorithm 21.7) or a subspace identification method (discussed in Chapter 23, Algorithm 23.4) with the user-specified order, delay and K = 0 can be used.
        ```matlab
        % Using Steiglitz-McBride method
        oe_stmcb = stmcb(data, nb, nf, nk); ?
        ```
   - The subspace identification may be the most preferred since it also facilitates "automatic" estimation of the order, when it is not specified.
        ```matlab
        % Using IV method
        bj_iv = iv4('bj', data, [nb nc nd nf nk]);
        ```

3. **BJ models**: 
   - Once again the multistage IV method with the specified orders may be used here. 
   - An alternative strategy consists of first fitting an OE model of the requisite order followed by a time-series model fit to the residuals, which also provides a good initial guess. This is a two-stage method. 
   - However, the OE model estimation itself may require an initial guess as described above. 
   - This procedure is illustrated in the case study.
        ```matlab
        % Two-stage method
        oe_model = oe(data, [nb nf nk]);
        residuals = resid(oe_model, data);
        arma_model = armax(residuals, [na nc]);
        bj_two_stage = idpoly(1, oe_model.b, arma_model.c, arma_model.a, oe_model.f, 'nk', nk);
        ```

The smae implementations in python:

```python
import numpy as np
from scipy import signal
import control

# Assuming 'data' is a tuple or list containing (y, u, Ts) where y is output, u is input, and Ts is sampling time

# 1. ARMAX models
# Using IV method (approximation)
def iv4_armax(data, orders):
    y, u, Ts = data
    na, nb, nc, nk = orders
    sys = control.arx(data, [na, nb, nk])
    return control.tf2ss(sys)

armax_iv4 = iv4_armax(data, [na, nb, nc, nk])

# 2. OE models
# Using Steiglitz-McBride method
def stmcb_oe(data, orders):
    y, u, Ts = data
    nb, nf, nk = orders
    b, a = signal.stmcb(u, y, nb, nf)
    return control.tf2ss(control.tf(b, a, dt=Ts))

oe_stmcb = stmcb_oe(data, [nb, nf, nk])

# Using subspace identification (approximation)
def n4sid_oe(data, order):
    y, u, Ts = data
    sys = control.ss(control.era(y, u, order))
    return sys

oe_n4sid = n4sid_oe(data, order)

# 3. BJ models
# Two-stage method
def bj_two_stage(data, orders):
    y, u, Ts = data
    nb, nc, nd, nf, nk = orders
    
    # First stage: OE model
    oe_model = control.oe(data, [nb, nf, nk])
    
    # Calculate residuals
    _, y_pred, _ = control.forced_response(oe_model, T=np.arange(len(u))*Ts, U=u)
    residuals = y - y_pred
    
    # Second stage: ARMA model on residuals
    arma_model = control.armax([residuals, np.zeros_like(residuals), Ts], [na, 0, nc])
    
    # Combine OE and ARMA models
    bj_model = control.tf2ss(control.parallel(oe_model, arma_model))
    return bj_model

bj_two_stage = bj_two_stage(data, [nb, nc, nd, nf, nk])

# Note: Replace na, nb, nc, nd, nf, nk, and order with appropriate values
```
## ORDER DETERMINATION

n appropriate model order for an identification problem can be determined in different ways, but information-theoretic measures are widely used for this purpose. It is also a good practice to choose an initial order using methodical ways rather than an arbitrary guess.

### Preliminary estimates of order

Depending on the situation and the type of model, different methods can be employed:

1. **Noise models**: ACF and PACF provide good indications of orders for MA and AR models. They also provide bounds on ARMA model orders.

2. **Plant models**: Examination of non-parametric model estimates can offer some clues. For instance:
   - If the step response estimates show underdamped characteristics, clearly a second-order model is a good starting point. 
   - The Bode plots also offer some insights - for example, the phase and roll-off in the magnitude plots at high frequencies are indicative of the order. 
   - However, more often than not, these are of limited help.

One of the best initial guesses of the overall plant and noise model orders are offered by subspace identification methods (described in Chapter 23). This implies they can be used for identification of time-series models as well.

The final choice of order is guided by:
- Residual analysis
- Covariance of parameter estimates
- Information-theoretic criteria such as Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), etc. that are presented next.

### Matlab implementation

```matlab
% MATLAB Code

% Assuming 'data' is your iddata object containing input-output data
y = data.y;
u = data.u;
Ts = data.Ts;

% 1. Noise Models
% ACF and PACF for AR/MA order estimation
[acf, lags] = autocorr(y);
[pacf, ~] = parcorr(y);

figure;
subplot(2,1,1);
stem(lags, acf);
title('Autocorrelation Function');
subplot(2,1,2);
stem(lags, pacf);
title('Partial Autocorrelation Function');

% 2. Plant Models
% Step response
sys = tfest(data, 2);  % Estimate 2nd order transfer function
step(sys);
title('Step Response');

% Bode plot
figure;
bode(sys);
title('Bode Plot');

% 3. Subspace Identification
n = 10;  % Maximum order to consider
orders = 1:n;
fits = zeros(size(orders));

for i = orders
    m = n4sid(data, i);
    fits(i) = compare(data, m);
end

figure;
plot(orders, fits);
title('Model Fit vs Order');
xlabel('Model Order');
ylabel('Fit (%)');

% 4. Information Criteria
[aic, bic] = aic_bic(data, orders);

figure;
plot(orders, aic, 'b-', orders, bic, 'r--');
legend('AIC', 'BIC');
title('Information Criteria');
xlabel('Model Order');
ylabel('Criterion Value');

function [aic, bic] = aic_bic(data, orders)
    N = length(data.y);
    aic = zeros(size(orders));
    bic = zeros(size(orders));
    for i = 1:length(orders)
        m = arx(data, [orders(i) orders(i) 1]);
        e = resid(m, data);
        sse = sum(e.y.^2);
        aic(i) = log(sse/N) + 2*orders(i)/N;
        bic(i) = log(sse/N) + orders(i)*log(N)/N;
    end
end
```

### Python implementation

```python
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import acf, pacf
from scipy import signal
import control

# Assuming 'data' is a tuple or list containing (y, u, Ts) where y is output, u is input, and Ts is sampling time
y, u, Ts = data

# 1. Noise Models
# ACF and PACF for AR/MA order estimation
acf_values = acf(y)
pacf_values = pacf(y)

plt.figure(figsize=(10, 8))
plt.subplot(2,1,1)
plt.stem(acf_values)
plt.title('Autocorrelation Function')
plt.subplot(2,1,2)
plt.stem(pacf_values)
plt.title('Partial Autocorrelation Function')
plt.tight_layout()
plt.show()

# 2. Plant Models
# Step response
sys = control.tfest(u, y, 2, Ts)  # Estimate 2nd order transfer function
t, y_step = control.step_response(sys)

plt.figure()
plt.plot(t, y_step)
plt.title('Step Response')
plt.show()

# Bode plot
w, mag, _ = control.bode(sys, plot=True)
plt.title('Bode Plot')
plt.show()

# 3. Subspace Identification
n = 10  # Maximum order to consider
orders = range(1, n+1)
fits = []

for i in orders:
    sys = control.ss(control.era(y, u, i))
    _, y_pred, _ = control.forced_response(sys, T=np.arange(len(u))*Ts, U=u)
    fits.append(1 - np.linalg.norm(y - y_pred) / np.linalg.norm(y - np.mean(y)))

plt.figure()
plt.plot(orders, fits)
plt.title('Model Fit vs Order')
plt.xlabel('Model Order')
plt.ylabel('Fit')
plt.show()

# 4. Information Criteria
def aic_bic(y, u, orders):
    N = len(y)
    aic = []
    bic = []
    for order in orders:
        sys = control.arx((y, u, Ts), [order, order, 1])
        _, y_pred, _ = control.forced_response(sys, T=np.arange(len(u))*Ts, U=u)
        e = y - y_pred
        sse = np.sum(e**2)
        aic.append(np.log(sse/N) + 2*order/N)
        bic.append(np.log(sse/N) + order*np.log(N)/N)
    return aic, bic

aic, bic = aic_bic(y, u, orders)

plt.figure()
plt.plot(orders, aic, 'b-', label='AIC')
plt.plot(orders, bic, 'r--', label='BIC')
plt.legend()
plt.title('Information Criteria')
plt.xlabel('Model Order')
plt.ylabel('Criterion Value')
plt.show()

```
