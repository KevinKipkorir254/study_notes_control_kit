# MODEL QUALITY ASSESSMENT AND VALIDATION

The identified model is assessed in two phases:
1. Training phase
2. Test phase

## TRAINING PHASE

The objective is to determine if the model has captured the characteristics of training data with reasonable accuracy and reliability, i.e., if:

1. The fit is satisfactory (test for unbiasedness)
2. The errors in parameter estimates are low (test for variance and no over-parametrization has occurred)
It may be recalled that in several illustrative examples of previous chapters, models were tested for this property through:
- (Residual) correlation plots
- (Parameter estimation) error analysis
A formal discussion now follows.
The assessment consists of three components, in that order:

1. **Tests for model bias via statistical analysis of residuals:**

   A good model is one which has rightly captured all input effects and the predictable portions of the residuals $$y - Gu$$. In view of these expectations, the key requirements for an acceptable model are that the residuals:
   
   (a) should be uncorrelated with the input, and 
   (b) should not exhibit any temporal correlation. 
   
   In other words, there is no residual information left for the model to capture.

   a. **Test for bias in plant model G:**
      
      The first requirement above translates to insignificant cross-correlation between residuals and inputs, i.e.,

      $$\rho_{ϵu}[l] \approx 0; \quad \forall l \quad (22.70)$$

      The insignificance of $$\rho_{ϵu}[l] = 0$$ is tested statistically using the $$100(1 - \alpha)$$% (typically $$\alpha = 0.05$$ or $$0.01$$) significance levels for the CCF. For this purpose, the theory outlined in §16.4 is used. 

      While this is a routine step in identification, the technically correct way of implementing the procedure is to pre-whiten either the residuals or the input prior to the computation of cross-correlation and the significance levels. Recall Example 16.1, which showed how one is led to false inferences by not doing so. On the other hand, if the input or residual $$ϵ[k]$$ is white, then the pre-whitening step is not necessary. See also Example 22.4 below.

      When the residuals pass the cross-correlation test, we conclude that the characteristics of the plant G have been adequately identified.

   b. **Test for bias in noise model H:**
      
      The criterion for noise model to pass the bias test is straightforward - the residuals should have white noise characteristics. Therefore, one needs to apply the whiteness tests discussed in §16.4. The Box-Ljung statistic or its modified version defined in (16.45) and (16.46), respectively, are widely used for this purpose.

   **Remarks:** When the model under analysis belongs to the OE class, it is sufficient to subject the model to test for significance in cross-correlation alone since there is no provision in the structure to explain noise models.

   To summarize, the goodness (unbiasedness) of plant and noise models are tested through a proper correlation analysis of the residuals. If the estimated models are unbiased, the CCF $$\rho_{ϵu}[l]$$ (pre-whitened input) and the ACF $$\rho_{ϵϵ}[l]$$ should be insignificant at all lags.

   Impulse response analysis of model-error-model offers an alternative way of testing for the unbiasedness in plant models. In addition, the presence of feedback may also be tested by the foregoing methods.

2. **Test for variance of models via error analysis of estimates:** 

   While the previous step tests for bias in the estimated models, it is important to examine whether the unbiasedness has been achieved at the cost of variance. This constitutes the purpose of analyzing the errors in parameter estimates. 

   It is important that this second check is carried out only after the model has passed the correlation analysis test above. This is necessary since the expressions for estimation error computations in Chapters 14, 15 and 21, it may be recalled, assume that the functional form has been adequately captured and that the residuals are white. The prime requirement for a model to pass this test is that the (standard) errors should be small relative to the estimated values. In other words, none of the confidence intervals for the individual unknowns (parameters) should include a zero. As described in §13.12, this is equivalent to a hypothesis test of $$H_0 : \theta_i = 0; \quad \forall i = 1, \ldots, p$$. The reader may recall that in several illustrative examples we have used this check for over-parametrization. Recall Example 14.5 on FIR estimation using the least squares method in this context.

3. **Evaluation of model responses:** 

   Once the model has passed the bias and variance tests above, it is useful to simulate the model responses - both elementary (impulse, step and frequency) and those to the inputs used in training. For the former, the model response may be compared with the non-parametric estimates while the response to input may be compared to the observed output. The fits may be assessed both by visual examination and quantitative metrics (for example, NRMS in (14.46)).

A model that has successfully passed the above-mentioned tests in the training phase is ready to be examined on a test data.

## TESTING PHASE

As remarked earlier, the main objective of this test is to assess the model's predictive abilities on a fresh data set. The prime purpose of this test, known as cross-validation, is to evaluate the model's extrapolation capabilities. The performance can be tested in two different ways:

- **Finite-step prediction**: Usually models are trained for delivering good one-step (or a finite-step) ahead predictions. Therefore, at the least, an acceptable model should perform well in this respect.

- **Infinite-step prediction**: This is the case of simulation as was discussed in §18.4. In the absence of any end-use requirements, this is a preferred test for any model. Since the infinite step ahead prediction of any stochastic model is zero, noise models should not be subjected to this test.

If a model performs satisfactorily on the training data but fails the cross-validation test, then there could be two reasons:

1. **Overfitting**: The model has captured variations that are local to the training data rather than actually capturing the global characteristics of the process.

2. **Improper test data**: The test data may have features that are distinctly different from what the model has seen in the training data.

While the first cause requires a re-evaluation of model's overparametrization, the second cause calls for appropriate fresh data.

### MATLAB IMPLEMENTATION

```matlab
% MATLAB Code

% Assuming 'data_train' and 'data_test' are iddata objects for training and test data
% 'model' is the identified model (e.g., an arx or oe model)

% 1. Tests for model bias via statistical analysis of residuals
e = resid(model, data_train);

% a. Test for bias in plant model G
[Ree, lags] = crosscorr(e.y, data_train.u);
figure;
crosscorr(e.y, data_train.u);
title('Cross-correlation between residuals and input');

% b. Test for bias in noise model H
figure;
autocorr(e.y);
title('Autocorrelation of residuals');

% 2. Test for variance of models
present(model);  % Display model with standard errors

% 3. Evaluation of model responses
figure;
compare(data_train, model);
title('Model vs Measured Output (Training Data)');

% Testing phase
% Finite-step prediction
[y_pred, fit] = compare(data_test, model);
figure;
compare(data_test, model);
title('Model vs Measured Output (Test Data)');

% Infinite-step prediction (simulation)
y_sim = sim(model, data_test);
figure;
plot(data_test.SamplingInstants, data_test.y, 'b', data_test.SamplingInstants, y_sim.y, 'r');
legend('Measured', 'Simulated');
title('Infinite-step ahead prediction');

% Quantitative metric (e.g., NRMSE)
nrmse = goodnessOfFit(y_sim.y, data_test.y, 'NRMSE');
disp(['NRMSE for infinite-step prediction: ', num2str(nrmse)]);
```

### PYTHON IMPLEMENTATION

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
import control
from statsmodels.tsa.stattools import acf, ccf
from sklearn.metrics import mean_squared_error

# Assuming 'data_train' and 'data_test' are tuples (y, u, Ts) for training and test data
# 'model' is the identified model (e.g., a state-space model from control.ss())

# 1. Tests for model bias via statistical analysis of residuals
_, y_pred, _ = control.forced_response(model, T=np.arange(len(data_train[0]))*data_train[2], U=data_train[1])
e = data_train[0] - y_pred

# a. Test for bias in plant model G
lags = np.arange(-20, 21)
ccf_eu = ccf(e, data_train[1], adjusted=False)
plt.figure()
plt.stem(lags, ccf_eu[20-len(lags):20+len(lags)+1])
plt.title('Cross-correlation between residuals and input')
plt.show()

# b. Test for bias in noise model H
acf_e = acf(e)
plt.figure()
plt.stem(range(len(acf_e)), acf_e)
plt.title('Autocorrelation of residuals')
plt.show()

# 2. Test for variance of models
# This would typically involve examining the covariance matrix of the parameter estimates
# The exact implementation depends on how the model was estimated

# 3. Evaluation of model responses
_, y_pred_train, _ = control.forced_response(model, T=np.arange(len(data_train[0]))*data_train[2], U=data_train[1])
plt.figure()
plt.plot(data_train[0], 'b', label='Measured')
plt.plot(y_pred_train, 'r', label='Predicted')
plt.legend()
plt.title('Model vs Measured Output (Training Data)')
plt.show()

# Testing phase
# Finite-step prediction
_, y_pred_test, _ = control.forced_response(model, T=np.arange(len(data_test[0]))*data_test[2], U=data_test[1])
plt.figure()
plt.plot(data_test[0], 'b', label='Measured')
plt.plot(y_pred_test, 'r', label='Predicted')
plt.legend()
plt.title('Model vs Measured Output (Test Data)')
plt.show()

# Infinite-step prediction (simulation)
y_sim = control.simulate_system(model, T=np.arange(len(data_test[0]))*data_test[2], U=data_test[1])
plt.figure()
plt.plot(data_test[0], 'b', label='Measured')
plt.plot(y_sim, 'r', label='Simulated')
plt.legend()
plt.title('Infinite-step ahead prediction')
plt.show()

# Quantitative metric (e.g., NRMSE)
def nrmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred)) / np.std(y_true)

nrmse_value = nrmse(data_test[0], y_sim)
print(f'NRMSE for infinite-step prediction: {nrmse_value}')
```

