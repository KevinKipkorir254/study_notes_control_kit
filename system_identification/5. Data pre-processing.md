# DATA PRE-PROCESSING

When the data have been collected from the identification experiment, they are not
likely to be in shape for immediate use in identification algorithms. There are several
possible deficiencies in the data that should be attended to:
1. High-frequency disturbances in the data record, above the frequencies of interest
   to the system dynamics
2. Occasional bursts and outliers, missing data, non-continuous data records
3. Drift and offset, low-frequency disturbances, possibly of periodic character

It must be stressed that in off-line applications, one should always first plot the data
in order to inspect them for these deficiencies. In this section we shall discuss how
to preprocess the data so as to avoid problems in the identification procedures later.

## DRIFTS AND DETRENDING

Low-frequency disturbances, offsets, trends, drift, and periodic (seasonal) variations
are not uncommon in data. They typically stem from external sources that we may
or may not prefer to include in the modeling. There are basically two different
approaches to dealing with such problems:
 1. Removing the disturbances by explicit pretreatment of the data
 2. Letting the noise model take care of the disturbances

The first approach involves removing trends and offsets by direct subtraction,
while the second relies on noise models with poles on or close to the unit circle,
like the ARIMA models much used in the Box and Jenkins approach.

### Signal offsets

We shall illustrate the two approaches applied to the offset problem. The standard
linear models that we use. like,

$$A(q)y(t) = B(q)u(t) + v(t)$$  (1)

describe the relationship between it u and y. There are at least six ways to deal with
this problem:

1. Let $$y(t)$$ and $$u(t)$$ be deviations from a physical equilibrium: 
The most natural approach is to determine the level $$\bar{y}$$ that corresponds to a constant $$u_m(t) = \bar{u}$$ close to the desired operating point. Then define

$$y(t) = y_m(t) - \bar{y}$$ (3a)

$$u(t) = u_m(t) - \bar{u}$$ (3b)

as the deviations from this equilibrium. These translated variables will automatically satisfy (14.2), making both members equal to zero, and (2) will thus not influence the fit in (1). This approach emphasizes the physical interpretation of (1) as a linearization around the equilibrium.

2. Subracting sample means: A sound approach is to define

$$\bar{u} = \frac{1}{N} \sum_{t=1}^N u_m(t)$$ (4a)

$$\bar{y} = \frac{1}{N} \sum_{t=1}^N y_m(t)$$ (4b)

and then use (14.3). If an input $$u_m(t)$$ that varies around $$\bar{u}$$ leads to an output that varies around $$\bar{y}$$, then $$(\bar{u}, \bar{y})$$ is likely to be close to an equilibrium point of the system. Approach 2 is thus closely related to the first approach.

3. Estimating the offset: One could also model the system using variables in the original physical units and add a constant that takes care of the offsets:

$$A(q)y_m(t) = B(q)u_m(t) + a + v(t)$$ (5) 

Comparing with (1) to (3), we see that $$a$$ corresponds to $$A(1)\bar{y} - B(1)\bar{u}$$. The value $$a$$ is then included in the parameter vector $$\theta$$ and estimated from data. It turns out that this approach in fact is a slight variant of the second approach.

4. Using a noise model with integration (≡ differencing the data): In (5) the constant $$a$$ could be viewed as a constant disturbance, which is modeled as

$$a\delta(t) = \frac{a}{1 - q^{-1}}$$ (6)

where $$\delta(t)$$ is the unit pulse at time zero. The model then reads

$$y_m(t) = \frac{B(q)}{A(q)}u_m(t) + \frac{1}{(1 - q^{-1})A(q)}w(t)$$ (7)

where $$w(t)$$ is the combined noise source $$a\delta(t) + v(t) - v(t - 1)$$. The offset $$a$$ can thus be described by changing the noise model from $$1/A(q)$$ to $$1/[(1 - q^{-1})A(q)]$$. According to what we noted in (7.14), this is equivalent to prefiltering the data through the filter $$L(q) = 1 - q^{-1}$$, that is, differencing the data:

$$y_f(t) = L(q)y_m(t) = y_m(t) - y_m(t - 1)$$ (8a)

$$u_f(t) = L(q)u_m(t) = u_m(t) - u_m(t - 1)$$ (8b)

5. Extending the noise model: Notice that the model (7) becomes a special case of (1) if the orders of the       $$A$$ and $$B$$ polynomials in (1) are increased by 1. Then a common factor $$1 - q^{-1}$$ can be included in  $$A (q)$$ and $$B(q)$$. This means that a higher-order model, when applied to the raw data $$y_m$$, will converge to a model like (7).

6. High pass filter: Differencing data is a rather drastic filter for removing
   a static component. Any high-pass filter that has gain (close to) zero at frequency
   zero will have the same effect

## OUTLIERS AND MISSING DATA

In practice, the data acquisition equipment is not perfect. It may be that single values
or portions of the input-output data are missing, due to malfunctions in the sensors
or communication links. It may also be that certain measured values are in obvious
error due to measurement failures. Such bad values are often called outliers, and
may have a substantial negative effect on the estimate. Bad values are often much
easier to detect in a residual plot
To deal with outliers and missing data, there are a few possibilities. One is to
cut out segments of the data sequence so that portions with bad data are avoided.
The segments can then be merged using the techniques of Section 14.3. For a data
set with many inputs and outputs it might be difficult—in certain applications—to
find data segments that are "clean" in all variables. It is then better to treat outliers,
both in inputs and outputs, as missing data and view them as unknown parameters.

### Dealing with missing data

Assume, for the moment, that we have a model $$M(\theta)$$ that describes the relationship between the input-output data. In the basic linear predictor form (4.6) it is given by

$$\hat{y}(t|\theta) = \sum_{k=1}^{p} g(t - k, \theta) u(k) + \sum_{k=1}^{m} h(t - k, \theta) y(k)$$ (11)

Suppose now that some of the input-output data are missing. One must distinguish between missing inputs and missing outputs, since they should be handled differently. Let us first consider missing inputs.

#### Missing input data  
If the input is a deterministic sequence, it is natural to consider missing inputs as unknown parameters. Since the above expression is linear in the data, it is clear that for a given model $$M(\theta)$$ the missing data can be estimated using a linear regression, least squares procedure. If we denote the missing data with the vector $$\eta$$, we have:

$$\hat{y}(t|\theta, \eta) = \sum_{k \in \mathcal{A}} g(t - k, \theta) u(k) + \varphi^T(t, \eta) + \sum_{k=1}^{l} h(t - k, \theta) y(k)$$ (12)

where $$k \in \mathcal{K}_u$$ is the set of non-missing inputs $$u(k)$$, and $$\varphi^T(t, \eta)$$ is made up from $$g(t - k, \theta)$$, $$k \in \mathcal{K}_u$$ in an obvious way. The parameters $$\theta$$ and $$\eta$$ can then be estimated by a prediction error criterion in the usual way. Note that for fixed $$\theta$$, (14.12) is a linear regression for $$\eta$$, so missing input data can easily be estimated for any given model. It may then be natural (but not necessarily numerically efficient) to iterate between estimating the missing data, using the current model, i.e., estimating $$\eta$$ for fixed $$\theta$$, and estimating the model $$\theta$$ using the currently reconstructed missing data. To start up the iterations, the first model can be built using linearly interpolated values for the missing data.

#### Missing output data

It is not natural to regard missing output data as unknown parameters, since they are treated as random variables in the prediction framework. The correct prediction error criterion will be to minimize the error between $$y(t)$$ and $$\hat{y}(t|\theta, Y^t)$$, where the prediction is based on those past $$y(k)$$ that actually have been observed ($$k \in \mathcal{K}_y$$). To compute this prediction correctly we can use the time-varying Kalman filter (4.94)-(4.95) and deal with the missing data as irregular sampling. To be more specific, suppose that the underlying, discrete time model is given in innovations form as:

$$x(t + 1, \theta) = A(\theta)x(t, \theta) + B(\theta)u(t) + K(\theta)e(t)$$ (13)

$$y(t) = C(\theta)x(t, \theta) + e(t)$$ (14)

The cross-covariance between process noise and measurement noise (see (4.85)) will be $$R_{12}(0) = K(0)R_2$$. Now, if some or all components of $$y(t)$$ are missing at a certain time $$t$$, this is treated as time-varying $$C(t,\theta)$$ and $$R_2(t,\theta)$$, where only those rows of $$C$$ and $$R_{12}$$ are extracted, that correspond to measured outputs. If all outputs are missing at time $$t$$, $$C_t$$ and $$R_{t,12}$$ will be the empty matrices. The time varying Kalman filter (4.94)-(4.95) with $$C_t(\theta)$$ and $$R_{t,12}(\theta)$$ inserted into (4.95) will now produce the correct predictors.

$$y(t|\theta), C(\theta), x(t,\theta), and \hat{y}(t|\theta, Y_k)$$

Working with the time varying predictor of course leads to much more computations, and approximate alternatives sometimes may be preferable. One approximation would be to replace any missing $$y(k)$$ in (14.11) by $$\hat{y}(k|\theta)$$, i.e., the predictor based on data up to time $$k - 1$$. (The correct replacement would be to use the smoothed estimate of $$y(k)$$ using measured data up to time $$t - 1$$.) Another approximation would be to treat also missing outputs as unknown parameters. This corresponds to replacing missing $$y(k)$$ in (14.11) by their smoothed estimates, using the whole data record. A third possibility is to carry out the minimization of the prediction error criterion by the EM-method, see Problem 10G.3. The missing data then correspond to the auxiliary measurements $$X$$.




