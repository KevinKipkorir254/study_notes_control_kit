# DATA PRE-PROCESSING

When the data have been collected from the identification experiment, they are not
likely to be in shape for immediate use in identification algorithms. There are several
possible deficiencies in the data that should be attended to:
1. High-frequency disturbances in the data record, above the frequencies of interest
   to the system dynamics
2. Occasional bursts and outliers, missing data, non-continuous data records
3. Drift and offset, low-frequency disturbances, possibly of periodic character

It must be stressed that in off-line applications, one should always first plot the data
in order to inspect them for these deficiencies. In this section we shall discuss how
to preprocess the data so as to avoid problems in the identification procedures later.

## DRIFTS AND DETRENDING

Low-frequency disturbances, offsets, trends, drift, and periodic (seasonal) variations
are not uncommon in data. They typically stem from external sources that we may
or may not prefer to include in the modeling. There are basically two different
approaches to dealing with such problems:
 1. Removing the disturbances by explicit pretreatment of the data
 2. Letting the noise model take care of the disturbances

The first approach involves removing trends and offsets by direct subtraction,
while the second relies on noise models with poles on or close to the unit circle,
like the ARIMA models much used in the Box and Jenkins approach.

### Signal offsets

We shall illustrate the two approaches applied to the offset problem. The standard
linear models that we use. like, $$A(q)y(t) = B(q)u(t) + v(t)$$

describe the relationship between it u and y. There are at least six ways to deal with
this problem:

1. Let $$y(t)$$ and $$u(t)$$ be deviations from a physical equilibrium: 
The most natural approach is to determine the level $$\bar{y}$$ that corresponds to a constant $$u_m(t) = \bar{u}$$ close to the desired operating point. Then define $$y(t) = y_m(t) - \bar{y}$$ (3a) $$u(t) = u_m(t) - \bar{u}$$ (3b)

as the deviations from this equilibrium. These translated variables will automatically satisfy (14.2), making both members equal to zero, and (2) will thus not influence the fit in (1). This approach emphasizes the physical interpretation of (1) as a linearization around the equilibrium.

2. Subracting sample means: A sound approach is to define $$\bar{u} = \frac{1}{N} \sum_{t=1}^N u_m(t)$$ (4a) $$\bar{y} = \frac{1}{N} \sum_{t=1}^N y_m(t)$$ (4b)

and then use (14.3). If an input $$u_m(t)$$ that varies around $$\bar{u}$$ leads to an output that varies around $$\bar{y}$$, then $$(\bar{u}, \bar{y})$$ is likely to be close to an equilibrium point of the system. Approach 2 is thus closely related to the first approach.

3. Estimating the offset: One could also model the system using variables in the original physical units and add a constant that takes care of the offsets:

        $$A(q)y_m(t) = B(q)u_m(t) + a + v(t)$$ (5) 

Comparing with (1) to (3), we see that $$a$$ corresponds to $$A(1)\bar{y} - B(1)\bar{u}$$. The value $$a$$ is then included in the parameter vector $$\theta$$ and estimated from data. It turns out that this approach in fact is a slight variant of the second approach.

4. Using a noise model with integration (≡ differencing the data): In (5) the constant $$a$$ could be viewed as a constant disturbance, which is modeled as $$a\delta(t) = \frac{a}{1 - q^{-1}}$$ (6)

where $$\delta(t)$$ is the unit pulse at time zero. The model then reads $$y_m(t) = \frac{B(q)}{A(q)}u_m(t) + \frac{1}{(1 - q^{-1})A(q)}w(t)$$ (7)

where $$w(t)$$ is the combined noise source $$a\delta(t) + v(t) - v(t - 1)$$. The offset $$a$$ can thus be described by changing the noise model from $$1/A(q)$$ to $$1/[(1 - q^{-1})A(q)]$$. According to what we noted in (7.14), this is equivalent to prefiltering the data through the filter $$L(q) = 1 - q^{-1}$$, that is, differencing the data: $$y_f(t) = L(q)y_m(t) = y_m(t) - y_m(t - 1)$$ (8a) $$u_f(t) = L(q)u_m(t) = u_m(t) - u_m(t - 1)$$ (8b)

5. Extending the noise model: Notice that the model (7) becomes a special case of (1) if the orders of the       $$A$$ and $$B$$ polynomials in (1) are increased by 1. Then a common factor $$1 - q^{-1}$$ can be included in  $$A (q)$$ and $$B(q)$$. This means that a higher-order model, when applied to the raw data $$y_m$$, will converge to a model like (7).

6. High pass filter: Differencing data is a rather drastic filter for removing
   a static component. Any high-pass filter that has gain (close to) zero at frequency
   zero will have the same effect

## OUTLIERS AND MISSING DATA

In practice, the data acquisition equipment is not perfect. It may be that single values
or portions of the input-output data are missing, due to malfunctions in the sensors
or communication links. It may also be that certain measured values are in obvious
error due to measurement failures. Such bad values are often called outliers, and
may have a substantial negative effect on the estimate. Bad values are often much
easier to detect in a residual plot
To deal with outliers and missing data, there are a few possibilities. One is to
cut out segments of the data sequence so that portions with bad data are avoided.
The segments can then be merged using the techniques of Section 14.3. For a data
set with many inputs and outputs it might be difficult—in certain applications—to
find data segments that are "clean" in all variables. It is then better to treat outliers,
both in inputs and outputs, as missing data and view them as unknown parameters.

### Dealing with missing data

Assume, for the moment, that we have a model $$M(\theta)$$ that describes the relationship between the input-output data. In the basic linear predictor form (4.6) it is given by $$\hat{y}(t|\theta) = \sum_{k=1}^{p} g(t - k, \theta) u(k) + \sum_{k=1}^{m} h(t - k, \theta) y(k)$$ (11)

Suppose now that some of the input-output data are missing. One must distinguish between missing inputs and missing outputs, since they should be handled differently. Let us first consider missing inputs.

#### Missing input data  
If the input is a deterministic sequence, it is natural to consider missing inputs as unknown parameters. Since the above expression is linear in the data, it is clear that for a given model $$M(\theta)$$ the missing data can be estimated using a linear regression, least squares procedure. If we denote the missing data with the vector $$\eta$$, we have: $$\hat{y}(t|\theta, \eta) = \sum_{k \in \mathcal{A}} g(t - k, \theta) u(k) + \varphi^T(t, \eta) + \sum_{k=1}^{l} h(t - k, \theta) y(k)$$ (12)

where $$k \in \mathcal{K}_u$$ is the set of non-missing inputs $$u(k)$$, and $$\varphi^T(t, \eta)$$ is made up from $$g(t - k, \theta)$$, $$k \in \mathcal{K}_u$$ in an obvious way. The parameters $$\theta$$ and $$\eta$$ can then be estimated by a prediction error criterion in the usual way. Note that for fixed $$\theta$$, (14.12) is a linear regression for $$\eta$$, so missing input data can easily be estimated for any given model. It may then be natural (but not necessarily numerically efficient) to iterate between estimating the missing data, using the current model, i.e., estimating $$\eta$$ for fixed $$\theta$$, and estimating the model $$\theta$$ using the currently reconstructed missing data. To start up the iterations, the first model can be built using linearly interpolated values for the missing data.

#### Missing output data

It is not natural to regard missing output data as unknown parameters, since they are treated as random variables in the prediction framework. The correct prediction error criterion will be to minimize the error between $$y(t)$$ and $$\hat{y}(t|\theta, Y^t)$$, where the prediction is based on those past $$y(k)$$ that actually have been observed ($$k \in \mathcal{K}_y$$). To compute this prediction correctly we can use the time-varying Kalman filter (4.94)-(4.95) and deal with the missing data as irregular sampling. To be more specific, suppose that the underlying, discrete time model is given in innovations form as: $$x(t + 1, \theta) = A(\theta)x(t, \theta) + B(\theta)u(t) + K(\theta)e(t)$$ (13) $$y(t) = C(\theta)x(t, \theta) + e(t)$$ (14)

The cross-covariance between process noise and measurement noise (see (4.85)) will be $$R_{12}(0) = K(0)R_2$$. Now, if some or all components of $$y(t)$$ are missing at a certain time $$t$$, this is treated as time-varying $$C(t,\theta)$$ and $$R_2(t,\theta)$$, where only those rows of $$C$$ and $$R_{12}$$ are extracted, that correspond to measured outputs. If all outputs are missing at time $$t$$, $$C_t$$ and $$R_{t,12}$$ will be the empty matrices. The time varying Kalman filter (4.94)-(4.95) with $$C_t(\theta)$$ and $$R_{t,12}(\theta)$$ inserted into (4.95) will now produce the correct predictors. $$y(t|\theta), C(\theta), x(t,\theta), and \hat{y}(t|\theta, Y_k)$$

Working with the time varying predictor of course leads to much more computations, and approximate alternatives sometimes may be preferable. One approximation would be to replace any missing $$y(k)$$ in (14.11) by $$\hat{y}(k|\theta)$$, i.e., the predictor based on data up to time $$k - 1$$. (The correct replacement would be to use the smoothed estimate of $$y(k)$$ using measured data up to time $$t - 1$$.) Another approximation would be to treat also missing outputs as unknown parameters. This corresponds to replacing missing $$y(k)$$ in (14.11) by their smoothed estimates, using the whole data record. A third possibility is to carry out the minimization of the prediction error criterion by the EM-method, see Problem 10G.3. The missing data then correspond to the auxiliary measurements $$X$$.

## SELECTING SEGMENTS OF DATA AND MERGING EXPERIMENTS
### Selecting Data Segments

When data from an identification experiment or. in particular, from normal operating
records are plotted, it often happens that there are portions of bad data or nonrelevant
information. The reason could be that there are long stretches of missing
data which will be difficult or computationally costly to reconstruct. There could be
portions with disturbances that are considered to be non-representative, or that take
the process into operating points that are of less interest. In particular for normal
operating records, there could also be long periods of "no information:" nothing
seems to happen that carries any information about the/process dynamics. In these
cases it is natural to select segments of the original data set which are considered to
contain relevant information about dynamics of interest. The procedure of how to
select such segments will basically be subjective and will have to rely mostly upon
intuition and process insights.

### MERGING DATA SETS

It is also a very common situation in practice that a number of separate experiments have been performed. The reason could be that the plant is not available for long, continuous experiments, or that only one input at a time is allowed to be manipulated in separate experiments. A further reason is, as described above, that bad data have forced us to split up the data record into several separate segments. How shall such separate records be treated? We cannot simply concatenate the data segments, because the connection points would cause transients that may destroy the estimate.
Suppose we build a model for each of the data segments, all with the same structure. Let the parameter estimate for segment $$i$$ be denoted by $$\hat{\theta}^{(i)}$$, and let its estimated covariance matrix be $$P^{(i)}$$. Assume also that the segments are so well separated that the different estimates can be regarded as independent. It is then well known from basic statistics that the optimal way to combine these estimates (giving
a resulting estimate of smallest variance) is to weigh them according to their inverse
covariance matrices. $$\theta = p \sum_{r=1}^n [p^{r-1}]^{-1} \theta_r$$ 
               $$\mathbf{P} = \left[ \sum_{r=1}^n [p^{r-1}]^{-1} \right]^{-1}$$ (15) $$\mathbf{P}$$ will then also be the covariance matrix of the resulting estimate $$\theta$$.

### Averaging over Periodic Data

A different way of "merging" data sets is at hand when an experiment with a periodic input has been conducted. We noted that it is then advantageous to average the output signal over the periods, so that the condensed set consists of just one period of input-output data. This allows shorter data records and independent noise estimates.

## MATLAB IMPLEMENTATION
For our case study all we need is to remove the offset and split the dat into segments for training and testing.

```matlab
uk_detransed = detrend(uk(501:1000));
y_detransed = detrend(y(501:1000));

uk_det = detrend(uk(1001:end));
y_det = detrend(y(1001:end));
```

## PYTHON IMPLEMENTATION

```python
import numpy as np
from scipy import signal
import matplotlib.pyplot as plt

# Assuming uk and y are already defined numpy arrays

# Detrend the data
uk_detrended = signal.detrend(uk[500:1000])
y_detrended = signal.detrend(y[500:1000])

uk_det = signal.detrend(uk[1000:])
y_det = signal.detrend(y[1000:])

# Optional: Plotting the results
plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.plot(uk[500:1000])
plt.title('Original uk[500:1000]')

plt.subplot(2, 2, 2)
plt.plot(uk_detrended)
plt.title('Detrended uk[500:1000]')

plt.subplot(2, 2, 3)
plt.plot(y[500:1000])
plt.title('Original y[500:1000]')

plt.subplot(2, 2, 4)
plt.plot(y_detrended)
plt.title('Detrended y[500:1000]')

plt.tight_layout()
plt.show()
# Note: Python uses 0-based indexing, so we subtract 1 from the MATLAB indices
```
---
<sup>1</sup> Consider discussing pre-filtering in this context.